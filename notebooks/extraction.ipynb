{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Notebook\n",
    "---\n",
    "\n",
    "This notebook is used to extract features from the data. The audio is divided into windows of 1 second. The features extracted from each audio window are:\n",
    "- **MFCC (Mel-Frequency Cepstral Coefficients)**: n_mfcc chosen by the user\n",
    "- **Chroma STFT**: 12 coefficients\n",
    "- **CQT (Constant-Q Transform)**: n_cqt chosen by the user\n",
    "- **RMS (Root Mean Square) Energy**: 1 coefficient\n",
    "- **Spectral Centroid**: 1 coefficient\n",
    "- **Spectral Bandwidth**: 1 coefficient\n",
    "- **Spectral Roll-off**: 1 coefficient\n",
    "- **Zero Crossing Rate**: 1 coefficient\n",
    "\n",
    "#### Sections:\n",
    "- [Feature Extraction](#Feature-Extraction)\n",
    "  - [Specific Features Extraction](#Specific-Features-Extraction)\n",
    "  - [All Features Extraction](#All-Features-Extraction)\n",
    "\n",
    "  \n",
    "#### Findings:\n",
    "- We have highly unbalanced classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/andreaalberti/Desktop/Public_Projects/advanced-biomedical-project/notebooks/utils.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the functions\n",
    "import utils as utils\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import importlib\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "# import the utils module\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n",
       "\n",
       "# -------- tqdm DARK THEME --------\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>\n",
    "\n",
    "# -------- tqdm DARK THEME --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "#sr = ''\n",
    "sr = 4000\n",
    "\n",
    "# set the paths to data\n",
    "BASE_DIR = '../dataset/'\n",
    "ARTIFACTS_DIR = BASE_DIR + f'artifacts_{sr}/'\n",
    "EXTRAHLS_DIR = BASE_DIR + f'extrahls_{sr}/'\n",
    "MURMURS_DIR = BASE_DIR + f'murmurs_{sr}/'\n",
    "NORMALS_DIR = BASE_DIR + f'normals_{sr}/'\n",
    "EXTRASTOLES_DIR = BASE_DIR + f'extrastoles_{sr}/'\n",
    "\n",
    "# paths to save the features\n",
    "FEATURES_RAW_DIR = '../features/raw/'\n",
    "#FEATURES_RAW_DIR = \"../features/balanced/priori/\"\n",
    "\n",
    "PATHS = [ARTIFACTS_DIR, EXTRAHLS_DIR, MURMURS_DIR, NORMALS_DIR, EXTRASTOLES_DIR]\n",
    "\n",
    "# REMOVE THE AUGEMENTED SAMPLES WHEN EXTRACTING BASE RAW FEATURES\n",
    "# for path in PATHS:\n",
    "#      remove_generated_samples(path, 'USERAUGMENTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Features Extraction\n",
    "- **MFCC (Mel-Frequency Cepstral Coefficients)**: n_mfcc chosen by the user\n",
    "- **Chroma STFT**: 12 coefficients\n",
    "- **CQT (Constant-Q Transform)**: n_cqt chosen by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    dir_path: str,\n",
    "    label: str,\n",
    "    frame_length: float,\n",
    "    sample_rate: int = 44100,\n",
    "    n_mfcc: int = 13,\n",
    "    melkwargs: dict = {},\n",
    "    n_cqt: int = 84,\n",
    "    get_mfcc: bool = True,\n",
    "    get_chroma: bool = True,\n",
    "    get_cqt: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Extracts audio features (MFCC, chroma, RMS, spectral centroid, spectral bandwidth, spectral rolloff, zero-crossing rate)\n",
    "    from audio files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): The path to the directory containing audio files.\n",
    "        label (str): The label associated with the extracted features.\n",
    "        frame_length: The frame length for feature extraction.\n",
    "        sample_rate (int, optional): The sample rate of the audio files. Defaults to 44100 Hz.\n",
    "        n_mfcc (int, optional): The number of Mel-frequency cepstral coefficients (MFCCs) to extract. Defaults to 13.\n",
    "        melkwargs (dict, optional): Additional arguments for Mel spectrogram computation.\n",
    "        n_cqt (int, optional): The number of constant-Q transform (CQT) bins to extract. Defaults to 84.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list containing tensors of extracted features for each audio file.\n",
    "    \"\"\"\n",
    "    filenames = os.listdir(dir_path)\n",
    "    filenames = [file for file in filenames if not file.startswith(\".\")]\n",
    "    frame_length = int(1 / frame_length)\n",
    "\n",
    "    features = []  # List to store the features for all files\n",
    "\n",
    "    for file in tqdm(filenames, desc=f\"Extraction in progress\"):\n",
    "        audio, orig_sample_rate = torchaudio.load(os.path.join(dir_path, file))\n",
    "        orig_sample_rate_tmp = int(orig_sample_rate / frame_length)\n",
    "        audio_length = int(max(audio[0].shape) / (orig_sample_rate_tmp))\n",
    "\n",
    "        sample_rate = orig_sample_rate\n",
    "\n",
    "        # Reduce the audio from stereo to mono if needed\n",
    "        if audio.shape[0] > 1:\n",
    "            print(f\"Converting stereo audio to mono for {file}...\")\n",
    "            audio = torch.mean(audio, dim=0).reshape(1, -1)\n",
    "\n",
    "        features_local = []  # List to store the MFCC features for each file\n",
    "\n",
    "        for i in range(audio_length):\n",
    "            # Lazy load the audio, one sec at a time, to avoid memory issues\n",
    "            audio_mono = utils.slicing(\n",
    "                audio,\n",
    "                offset=int(sample_rate / frame_length * i),\n",
    "                num_frames=int(sample_rate / frame_length * (i + 1)),\n",
    "            )\n",
    "\n",
    "            # Get the MFCC features\n",
    "            if get_mfcc:\n",
    "                mfcc_features_tmp = utils.extract_mfcc(audio_mono, sample_rate, n_mfcc, melkwargs)\n",
    "            if get_chroma:\n",
    "                chroma_stft = utils.extract_chroma_stft(audio_mono, sample_rate)\n",
    "            if get_cqt:\n",
    "                cqt = utils.extract_cqt(audio_mono, sample_rate=sample_rate, n_cqt=n_cqt)\n",
    "\n",
    "          #   rms = extract_rms(audio_mono)\n",
    "          #   spec_cent = extract_spectral_centroid(audio_mono, sample_rate)\n",
    "          #   spec_bw = extract_spectral_bandwidth(audio_mono, sample_rate)\n",
    "          #   rolloff = extract_spectral_rolloff(audio_mono, sample_rate)\n",
    "          #   zcr = extract_zero_crossing_rate(audio_mono)\n",
    "\n",
    "            \n",
    "            # Concatenate the features\n",
    "            if get_mfcc:\n",
    "                features_tmp = mfcc_features_tmp\n",
    "            if get_chroma:\n",
    "                features_tmp = chroma_stft\n",
    "            if get_cqt:\n",
    "                features_tmp = cqt\n",
    "            \n",
    "            if get_mfcc and get_chroma:\n",
    "                features_tmp = torch.cat(\n",
    "                    (mfcc_features_tmp, chroma_stft),\n",
    "                    dim=0,\n",
    "                )\n",
    "            if get_mfcc and get_cqt:\n",
    "                features_tmp = torch.cat(\n",
    "                    (mfcc_features_tmp, cqt),\n",
    "                    dim=0,\n",
    "                )\n",
    "            if get_chroma and get_cqt:\n",
    "                features_tmp = torch.cat(\n",
    "                    (chroma_stft, cqt),\n",
    "                    dim=0,\n",
    "                )\n",
    "            if get_mfcc and get_chroma and get_cqt:\n",
    "                features_tmp = torch.cat(\n",
    "                    (mfcc_features_tmp, chroma_stft, cqt),\n",
    "                    dim=0,\n",
    "                )\n",
    "            if not get_mfcc and not get_chroma and not get_cqt:\n",
    "                features_tmp = torch.tensor([])\n",
    "\n",
    "            features_local.append(features_tmp)\n",
    "\n",
    "\n",
    "        # If features_local is empty, skip the file\n",
    "        if features_local == []:\n",
    "            print(f\"No features extracted for {file}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Stack the MFCC features into a single tensor\n",
    "        features_local = torch.stack(features_local, dim=0)\n",
    "\n",
    "        # Attach the label in the last column\n",
    "        features_local = torch.cat(\n",
    "            (features_local, torch.ones(features_local.shape[0], 1) * label), dim=1\n",
    "        )\n",
    "\n",
    "        # Attach the filename index in the last column\n",
    "        features_local = torch.cat(\n",
    "            (\n",
    "                features_local,\n",
    "                torch.full((features_local.shape[0], 1), filenames.index(file)),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        features.append(features_local)\n",
    "\n",
    "    print(\"Finished processing all files.\\n\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from ../dataset/artifacts/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77679e82638460082af52aa31df5574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo audio to mono for artifact_2023_17.wav...\n",
      "Converting stereo audio to mono for artifact_2023_16.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo audio to mono for artifact_2023_14.wav...\n",
      "Converting stereo audio to mono for artifact_2023_15.wav...\n",
      "Converting stereo audio to mono for artifact_2023_39.wav...\n",
      "Converting stereo audio to mono for artifact_2023_11.wav...\n",
      "Converting stereo audio to mono for artifact_2023_38.wav...\n",
      "Converting stereo audio to mono for artifact_2023_12.wav...\n",
      "Converting stereo audio to mono for artifact_2023_13.wav...\n",
      "Converting stereo audio to mono for artifact_2023_41.wav...\n",
      "Converting stereo audio to mono for artifact_2023_40.wav...\n",
      "Converting stereo audio to mono for artifact_2023_42.wav...\n",
      "Converting stereo audio to mono for artifact_2023_43.wav...\n",
      "Converting stereo audio to mono for artifact_2023_46.wav...\n",
      "Converting stereo audio to mono for artifact_2023_44.wav...\n",
      "Converting stereo audio to mono for artifact_2023_45.wav...\n",
      "Converting stereo audio to mono for artifact_2023_36.wav...\n",
      "Converting stereo audio to mono for artifact_2023_22.wav...\n",
      "Converting stereo audio to mono for artifact_2023_4.wav...\n",
      "Converting stereo audio to mono for artifact_2023_5.wav...\n",
      "Converting stereo audio to mono for artifact_2023_23.wav...\n",
      "Converting stereo audio to mono for artifact_2023_37.wav...\n",
      "Converting stereo audio to mono for artifact_2023_21.wav...\n",
      "Converting stereo audio to mono for artifact_2023_35.wav...\n",
      "Converting stereo audio to mono for artifact_2023_6.wav...\n",
      "Converting stereo audio to mono for artifact_2023_34.wav...\n",
      "Converting stereo audio to mono for artifact_2023_20.wav...\n",
      "Converting stereo audio to mono for artifact_2023_18.wav...\n",
      "Converting stereo audio to mono for artifact_2023_24.wav...\n",
      "Converting stereo audio to mono for artifact_2023_2.wav...\n",
      "Converting stereo audio to mono for artifact_2023_25.wav...\n",
      "Converting stereo audio to mono for artifact_2023_19.wav...\n",
      "Converting stereo audio to mono for artifact_2023_33.wav...\n",
      "Converting stereo audio to mono for artifact_2023_1.wav...\n",
      "Converting stereo audio to mono for artifact_2023_0.wav...\n",
      "Converting stereo audio to mono for artifact_2023_26.wav...\n",
      "Converting stereo audio to mono for artifact_2023_32.wav...\n",
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/artifacts/ features tensor is: (1199, 32)\n",
      "Extracting features from ../dataset/extrahls/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e960d635264e5b9ca066dade1054cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features extracted for speed_13USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for pitch_44USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for speed_20USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for pitch_12USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for extrahls__201104021355.wav. Skipping...\n",
      "No features extracted for noisy_3USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, PATH_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(PATHS):\n\u001b[1;32m     21\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting features from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \tfeatures \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_chroma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_cqt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \t\u001b[38;5;66;03m# Stack the features into a single tensor\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \tfeatures \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[42], line 59\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(dir_path, label, frame_length, sample_rate, n_mfcc, melkwargs, n_cqt, get_mfcc, get_chroma, get_cqt)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Get the MFCC features\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_mfcc:\n\u001b[0;32m---> 59\u001b[0m     mfcc_features_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_mfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_mono\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmelkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_chroma:\n\u001b[1;32m     61\u001b[0m     chroma_stft \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mextract_chroma_stft(audio_mono, sample_rate)\n",
      "File \u001b[0;32m~/Desktop/Public_Projects/advanced-biomedical-project/notebooks/utils.py:253\u001b[0m, in \u001b[0;36mextract_mfcc\u001b[0;34m(audio, sample_rate, n_mfcc, melkwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Create the MFCC transform\u001b[39;00m\n\u001b[1;32m    252\u001b[0m mfcc_transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mMFCC(sample_rate\u001b[38;5;241m=\u001b[39msample_rate, n_mfcc\u001b[38;5;241m=\u001b[39mn_mfcc, melkwargs\u001b[38;5;241m=\u001b[39mmelkwargs)\n\u001b[0;32m--> 253\u001b[0m mfcc_features \u001b[38;5;241m=\u001b[39m \u001b[43mmfcc_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(mfcc_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    256\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:707\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    704\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamplitude_to_DB(mel_specgram)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# (..., time, n_mels) dot (n_mels, n_mfcc) -> (..., n_nfcc, time)\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_specgram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdct_mat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mfcc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------- MFCC -------------------------\n",
    "n_mfcc_list = [30, 120]\n",
    "names = ['artifacts', 'extrahls', 'murmurs', 'normals', 'extrastoles']\n",
    "features_dict = {}\n",
    "interval = 1\n",
    "n_mfcc = 30\n",
    "type_ = 'mfcc'  # mfcc, mel_spec, spec, cqt\n",
    "# melkwargs = {'n_fft': 2048, 'hop_length': 512, 'n_mels': 128}\n",
    "\n",
    "for n_mfcc in n_mfcc_list:\n",
    "\t# Save the features to a file\n",
    "\tstoring_name = f'full_data_{interval}s_{sr}hz_{n_mfcc}{type_}'  ########## CHANGE THIS TO CHANGE THE NAME OF THE FILE\n",
    "\n",
    "\n",
    "\tif os.path.exists(FEATURES_RAW_DIR + storing_name + '.npy'):\n",
    "\t\tprint('The features have already been extracted')\n",
    "\t\t\n",
    "\telse:\n",
    "\t\tfor i, PATH_ in enumerate(PATHS):\n",
    "\t\t\t\n",
    "\t\t\tprint(f'Extracting features from {PATH_}')\n",
    "\t\t\tfeatures = extract_features(PATH_, label = i, frame_length = interval, n_mfcc=n_mfcc, get_mfcc=True, get_chroma=False, get_cqt=False)\n",
    "\t\t\t\n",
    "\t\t\t# Stack the features into a single tensor\n",
    "\t\t\tfeatures = torch.cat(features, dim=0).numpy()\n",
    "\t\t\tprint(f'The shape of the {PATH_} features tensor is: {features.shape}')\n",
    "\t\t\t\n",
    "\t\t\tX = features[:,:-2]\n",
    "\t\t\ty = features[:,-2]\n",
    "\t\t\tfilename = features[:,-1]\n",
    "\t\t\t\n",
    "\t\t\tlocal_dict = {'X': X, 'y': y, 'filename': filename}\n",
    "\t\t\t\n",
    "\t\t\tfeatures_dict[names[i]] = local_dict\n",
    "\t\t\t\t\n",
    "\t\t# Save the features to a file\n",
    "\t\tnp.save(FEATURES_RAW_DIR + storing_name, features_dict)\n",
    "\t\t\t\t\n",
    "\t\tprint('Features extracted and saved')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------- CHROMA -------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------- CQT -------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Features Extraction\n",
    "- **MFCC (Mel-Frequency Cepstral Coefficients)**: n_mfcc chosen by the user\n",
    "- **Chroma STFT**: 12 coefficients\n",
    "- **CQT (Constant-Q Transform)**: n_cqt chosen by the user\n",
    "- **RMS (Root Mean Square) Energy**: 1 coefficient\n",
    "- **Spectral Centroid**: 1 coefficient\n",
    "- **Spectral Bandwidth**: 1 coefficient\n",
    "- **Spectral Roll-off**: 1 coefficient\n",
    "- **Zero Crossing Rate**: 1 coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from ../dataset/artifacts/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6d80509a2744339b1b7ea4fbdc5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo audio to mono for artifact_2023_17.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo audio to mono for artifact_2023_16.wav...\n",
      "Converting stereo audio to mono for artifact_2023_14.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo audio to mono for artifact_2023_15.wav...\n",
      "Converting stereo audio to mono for artifact_2023_39.wav...\n",
      "Converting stereo audio to mono for artifact_2023_11.wav...\n",
      "Converting stereo audio to mono for artifact_2023_38.wav...\n",
      "Converting stereo audio to mono for artifact_2023_12.wav...\n",
      "Converting stereo audio to mono for artifact_2023_13.wav...\n",
      "Converting stereo audio to mono for artifact_2023_41.wav...\n",
      "Converting stereo audio to mono for artifact_2023_40.wav...\n",
      "Converting stereo audio to mono for artifact_2023_42.wav...\n",
      "Converting stereo audio to mono for artifact_2023_43.wav...\n",
      "Converting stereo audio to mono for artifact_2023_46.wav...\n",
      "Converting stereo audio to mono for artifact_2023_44.wav...\n",
      "Converting stereo audio to mono for artifact_2023_45.wav...\n",
      "Converting stereo audio to mono for artifact_2023_36.wav...\n",
      "Converting stereo audio to mono for artifact_2023_22.wav...\n",
      "Converting stereo audio to mono for artifact_2023_4.wav...\n",
      "Converting stereo audio to mono for artifact_2023_5.wav...\n",
      "Converting stereo audio to mono for artifact_2023_23.wav...\n",
      "Converting stereo audio to mono for artifact_2023_37.wav...\n",
      "Converting stereo audio to mono for artifact_2023_21.wav...\n",
      "Converting stereo audio to mono for artifact_2023_35.wav...\n",
      "Converting stereo audio to mono for artifact_2023_6.wav...\n",
      "Converting stereo audio to mono for artifact_2023_34.wav...\n",
      "Converting stereo audio to mono for artifact_2023_20.wav...\n",
      "Converting stereo audio to mono for artifact_2023_18.wav...\n",
      "Converting stereo audio to mono for artifact_2023_24.wav...\n",
      "Converting stereo audio to mono for artifact_2023_2.wav...\n",
      "Converting stereo audio to mono for artifact_2023_25.wav...\n",
      "Converting stereo audio to mono for artifact_2023_19.wav...\n",
      "Converting stereo audio to mono for artifact_2023_33.wav...\n",
      "Converting stereo audio to mono for artifact_2023_1.wav...\n",
      "Converting stereo audio to mono for artifact_2023_0.wav...\n",
      "Converting stereo audio to mono for artifact_2023_26.wav...\n",
      "Converting stereo audio to mono for artifact_2023_32.wav...\n",
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/artifacts/ features tensor is: (1199, 28)\n",
      "Extracting features from ../dataset/extrahls/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0615511d1fa42479bf6ca6cd973399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features extracted for speed_13USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for pitch_44USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for speed_20USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for pitch_12USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for extrahls__201104021355.wav. Skipping...\n",
      "No features extracted for noisy_3USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for speed_28USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "No features extracted for speed_21USERAUGMENTEDextrahls__201104021355.wav. Skipping...\n",
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/extrahls/ features tensor is: (883, 28)\n",
      "Extracting features from ../dataset/murmurs/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90672b7188504abc8b79365278c012f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features extracted for murmur__201104021355.wav. Skipping...\n",
      "Converting stereo audio to mono for abnormal_s4_2023_2.wav...\n",
      "No features extracted for murmur__171_1307971016233_E.wav. Skipping...\n",
      "Converting stereo audio to mono for atrial_septal_defect_2023_6.wav...\n",
      "Converting stereo audio to mono for abnormal_s3_2023_1.wav...\n",
      "Converting stereo audio to mono for mitral_stenosis_2023_9.wav...\n",
      "Converting stereo audio to mono for abnormal_s3_2023_0.wav...\n",
      "Converting stereo audio to mono for holosystolic_murmur_2023_7.wav...\n",
      "Converting stereo audio to mono for aortic_stenosis_2023_4.wav...\n",
      "Converting stereo audio to mono for mitral_valve_prolapse_2023_10.wav...\n",
      "Converting stereo audio to mono for innocent_murmur_2023_8.wav...\n",
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/murmurs/ features tensor is: (1149, 28)\n",
      "Extracting features from ../dataset/normals/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add9cd4d2df24d3da7ab3b226766dc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features extracted for normal__296_1311682952647_A1.wav. Skipping...\n",
      "Converting stereo audio to mono for normal_2023_3.wav...\n",
      "Converting stereo audio to mono for normal_2023_2.wav...\n",
      "Converting stereo audio to mono for normal_2023_0.wav...\n",
      "Converting stereo audio to mono for normal_2023_1.wav...\n",
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/normals/ features tensor is: (2161, 28)\n",
      "Extracting features from ../dataset/extrastoles/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4584860b0b4e859b4e2dacad37e174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction in progress:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing all files.\n",
      "\n",
      "The shape of the ../dataset/extrastoles/ features tensor is: (806, 28)\n",
      "Features extracted and saved\n"
     ]
    }
   ],
   "source": [
    "# FEATURES EXTRACTION\n",
    "names = ['artifacts', 'extrahls', 'murmurs', 'normals', 'extrastoles']\n",
    "features_dict = {}\n",
    "interval = 1\n",
    "n_mfcc = 30\n",
    "num_feat = 30\n",
    "type_ = 'mfcc'  # mfcc, mel_spec, spec, cqt\n",
    "# melkwargs = {'n_fft': 2048, 'hop_length': 512, 'n_mels': 128}\n",
    "\n",
    "# Save the features to a file\n",
    "storing_name = f'full_data_{interval}s_{sr}hz_{num_feat}{type_}'  ########## CHANGE THIS TO CHANGE THE NAME OF THE FILE\n",
    "\n",
    "\n",
    "if os.path.exists(FEATURES_RAW_DIR + storing_name + '.npy'):\n",
    "     print('The features have already been extracted')\n",
    "     \n",
    "else:\n",
    "     for i, PATH_ in enumerate(PATHS):\n",
    "          \n",
    "          print(f'Extracting features from {PATH_}')\n",
    "          features = extract_features(PATH_, label = i, frame_length = interval, n_mfcc=n_mfcc)\n",
    "          \n",
    "          # Stack the features into a single tensor\n",
    "          features = torch.cat(features, dim=0).numpy()\n",
    "          print(f'The shape of the {PATH_} features tensor is: {features.shape}')\n",
    "          \n",
    "          X = features[:,:-2]\n",
    "          y = features[:,-2]\n",
    "          filename = features[:,-1]\n",
    "          \n",
    "          local_dict = {'X': X, 'y': y, 'filename': filename}\n",
    "          \n",
    "          features_dict[names[i]] = local_dict\n",
    "               \n",
    "     # Save the features to a file\n",
    "     np.save(FEATURES_RAW_DIR + storing_name, features_dict)\n",
    "               \n",
    "     print('Features extracted and saved')\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the artifacts features tensor is: (1199, 26)\n",
      "The shape of the artifacts labels tensor is: (1199,)\n",
      "The shape of the artifacts filenames tensor is: (1199,)\n",
      "-----------------------------------------------\n",
      "The shape of the extrahls features tensor is: (883, 26)\n",
      "The shape of the extrahls labels tensor is: (883,)\n",
      "The shape of the extrahls filenames tensor is: (883,)\n",
      "-----------------------------------------------\n",
      "The shape of the murmurs features tensor is: (1149, 26)\n",
      "The shape of the murmurs labels tensor is: (1149,)\n",
      "The shape of the murmurs filenames tensor is: (1149,)\n",
      "-----------------------------------------------\n",
      "The shape of the normals features tensor is: (2161, 26)\n",
      "The shape of the normals labels tensor is: (2161,)\n",
      "The shape of the normals filenames tensor is: (2161,)\n",
      "-----------------------------------------------\n",
      "The shape of the extrastoles features tensor is: (806, 26)\n",
      "The shape of the extrastoles labels tensor is: (806,)\n",
      "The shape of the extrastoles filenames tensor is: (806,)\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load the features and check the shape\n",
    "features_dict = np.load(FEATURES_RAW_DIR + storing_name + '.npy', allow_pickle=True).item()\n",
    "for key in features_dict.keys():\n",
    "\tprint(f'The shape of the {key} features tensor is: {features_dict[key][\"X\"].shape}')\n",
    "\tprint(f'The shape of the {key} labels tensor is: {features_dict[key][\"y\"].shape}')\n",
    "\tprint(f'The shape of the {key} filenames tensor is: {features_dict[key][\"filename\"].shape}')\n",
    "\tprint('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6198"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for key in features_dict.keys():\n",
    "\ts += features_dict[key][\"X\"].shape[0]\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
