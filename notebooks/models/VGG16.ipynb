{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Training\n",
    "---\n",
    "\n",
    "#### Contents:\n",
    "- [Data Loading](#Data-Loading): Load the data from the proper directory.\n",
    "- [Model Training](#Model-Training): Train the model using the loaded data.\n",
    "  \n",
    "#### Findings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_NUM_TO_EXTRACT = {\n",
    "\t'mfcc': None,\n",
    "\t'spectrogram': None,\n",
    "\t'mel_spectrogram': None,\n",
    " }\n",
    "\n",
    "EXTRACTION_INTERVALS = [1]\n",
    "RANDOM_SEED = 42\n",
    "IMG_SHAPE = (80, 80)\n",
    "\n",
    "PATH_TO_SAVE =  '../../features/raw/'\n",
    "\n",
    "NAME = f'features_images_{EXTRACTION_INTERVALS[0]}_{IMG_SHAPE}_'\n",
    "for key in TYPE_NUM_TO_EXTRACT.keys():\n",
    "\tNAME += f'{key}_'\n",
    "NAME = NAME[:-1]\n",
    "NAME = NAME + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from a specific layer\n",
    "def extract_features(dataloader, model, layer_index=None):\n",
    "    if layer_index is None:\n",
    "        layer_index = len(model.features)\n",
    "\n",
    "    layer = model.features[:layer_index + 1]\n",
    "    layer.eval()\n",
    "    \n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(dataloader):\n",
    "            outputs = layer(inputs)\n",
    "            features.append(outputs.view(outputs.size(0), -1).cpu().numpy())\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def get_dataloaders_sampler(datasets, batch_size, sampler=None, shuffling=[True, False, False]):\n",
    "    \"\"\"\n",
    "    Get the dataloaders for the training, validation, and test sets. Use this function with sampler = True to balance the data.\n",
    "\n",
    "    Args:\n",
    "    - datasets (torch.utils.data.dataset.TensorDataset): The dataset containing the features of the real and fake audio files.\n",
    "    - batch_size (int): The batch size for the dataloaders.\n",
    "    - sampler (bool): A boolean value indicating whether to use a sampler for data balancing.\n",
    "    - shuffling (list, optional): A list of boolean values indicating whether to shuffle the data for each dataloader. The length of the list should be equal to the number of datasets.\n",
    "\n",
    "    Returns:\n",
    "    - dataloaders (list): A list containing the training, validation, and test dataloaders.\n",
    "    \"\"\"\n",
    "    \n",
    "    samplers = [None, None, None]\n",
    "    \n",
    "    # define a weighted random sampler to be used in the dataloader for data balancing\n",
    "    if sampler:\n",
    "        samplers = []\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                labels = dataset[:][1]\n",
    "            except: \n",
    "                labels = dataset.labels.long()\n",
    "            class_counts = torch.bincount(labels)\n",
    "            class_weights = 1. / class_counts.float()\n",
    "            weights = class_weights[labels]\n",
    "            sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "            samplers.append(sampler)\n",
    "            shuffling = [False, False, False]\n",
    "    \n",
    "    \n",
    "    dataloaders = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffling[i], sampler=samplers[i])\n",
    "        dataloaders.append(dataloader)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "# Function to count the number of samples per class in an epoch\n",
    "def count_samples_per_class(dataloader):\n",
    "    class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    for _, labels in dataloader:\n",
    "        for label in labels:\n",
    "            class_counts[label.item()] += 1\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, mfcc_data, labels, transform=None):\n",
    "        self.mfcc_data = mfcc_data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mfcc_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.mfcc_data[idx].numpy()\n",
    "        sample = np.stack([sample]*3, axis=0)  # Expand to 3 channels\n",
    "        sample = sample.transpose(1, 2, 0)  # Change to HWC format for PIL Image\n",
    "        sample = Image.fromarray((sample * 255).astype('uint8'))  # Convert to PIL Image\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, self.labels[idx]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ---------- MODELS ----------\n",
    "# Load the pre-trained VGG16_bn model\n",
    "vgg16_bn = models.vgg16_bn(pretrained=True)\n",
    "vgg16_bn.classifier = torch.nn.Identity()  # Remove the classifier to get features\n",
    "\n",
    "metrics_dict = {\n",
    "    'macrof1': f1_score,\n",
    "    'accuracy': accuracy_score,\n",
    "    'balanced_accuracy': balanced_accuracy_score\n",
    "}\n",
    "\n",
    "# ---------- STORING STRUCT ----------\n",
    "models_dict = {\n",
    "    \"rf\": RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "}\n",
    "\n",
    "# df having features type as columns and metrics as rows\n",
    "results_df = pd.DataFrame(columns=TYPE_NUM_TO_EXTRACT.keys(), index=metrics_dict.keys())\n",
    "\n",
    "# for each model save the df\n",
    "results_dict = { key: results_df.copy() for key in models_dict.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dbcb22c3eb4e43837a1b9d6bd95b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________ Train samples per class (interval 1):\t {0: 770, 1: 800, 2: 807, 3: 767, 4: 760} __________\n",
      "\n",
      "Extracting neural features from mfcc features\n",
      "\n",
      "Training rf on mfcc features\n",
      "\n",
      "rf macrof1 on mfcc features: 0.12402572221876222\n",
      "\n",
      "rf accuracy on mfcc features: 0.4098360655737705\n",
      "\n",
      "rf balanced_accuracy on mfcc features: 0.20096342787565552\n",
      "\n",
      "__________ Train samples per class (interval 1):\t {0: 782, 1: 785, 2: 772, 3: 770, 4: 795} __________\n",
      "\n",
      "Extracting neural features from spectrogram features\n",
      "\n",
      "Training rf on spectrogram features\n",
      "\n",
      "rf macrof1 on spectrogram features: 0.12274633405501115\n",
      "\n",
      "rf accuracy on spectrogram features: 0.41290983606557374\n",
      "\n",
      "rf balanced_accuracy on spectrogram features: 0.2021387682133359\n",
      "\n",
      "__________ Train samples per class (interval 1):\t {0: 747, 1: 782, 2: 806, 3: 747, 4: 822} __________\n",
      "\n",
      "Extracting neural features from mel_spectrogram features\n",
      "\n",
      "Training rf on mel_spectrogram features\n",
      "\n",
      "rf macrof1 on mel_spectrogram features: 0.12907804077879656\n",
      "\n",
      "rf accuracy on mel_spectrogram features: 0.4139344262295082\n",
      "\n",
      "rf balanced_accuracy on mel_spectrogram features: 0.20401614010419866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the features\n",
    "features = torch.load(PATH_TO_SAVE + NAME)\n",
    "\n",
    "for feature_type in tqdm(TYPE_NUM_TO_EXTRACT.keys()): #x3\n",
    "\n",
    "    # get the sets\n",
    "    X_train, y_train = features[feature_type]['train'][\"X\"], features[feature_type]['train'][\"y\"]\n",
    "    X_test, y_test = features[feature_type]['test'][\"X\"], features[feature_type]['test'][\"y\"]\n",
    "    \n",
    "    # create the datasets and balanced dataloaders\n",
    "    train_dataset = MFCCDataset(X_train, y_train, transform=transform)\n",
    "    train_dataloader = get_dataloaders_sampler([train_dataset], batch_size=32, sampler=True, shuffling=[True])[0]\n",
    "    # print the number of samples per class\n",
    "    print(f\"{'_'*10} Train samples per class (interval {EXTRACTION_INTERVALS[0]}):\\t {count_samples_per_class(train_dataloader)} {'_'*10}\\n\")\n",
    "    test_dataset = MFCCDataset(X_test, y_test, transform=transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)    \n",
    "    \n",
    "    print(f\"Extracting neural features from {feature_type} features\\n\")\n",
    "    train_features = extract_features(train_dataloader, vgg16_bn)\n",
    "    test_features = extract_features(test_dataloader, vgg16_bn)\n",
    "    \n",
    "    # train the models\n",
    "    for model_name, model in models_dict.items(): #x1\n",
    "        print(f\"Training {model_name} on {feature_type} features\\n\")\n",
    "        model.fit(train_features, y_train)\n",
    "        \n",
    "        # evaluate the model\n",
    "        for metric_name, metric in metrics_dict.items(): #x3\n",
    "            y_pred = model.predict(test_features)\n",
    "            if 'macro' in metric_name:\n",
    "                score = metric(y_test, y_pred, average='macro')\n",
    "            else:\n",
    "                score = metric(y_test, y_pred)\n",
    "                \n",
    "            print(f\"{model_name} {metric_name} on {feature_type} features: {score}\\n\")\n",
    "        \n",
    "            # save the results\n",
    "            results_dict[model_name].loc[metric_name, feature_type] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results_dict to a file\n",
    "torch.save(results_dict, PATH_TO_SAVE + 'results_dict_VGG16.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
