\section{Feature Selection and Model Training}

\subsection{Determining the Optimal Number of Features}

To maximize the model's performance, it is crucial to determine the optimal number of features for each type. This was achieved using a 'One Model per Feature' approach, which involved the following steps:

\begin{algorithm}
    \caption{Feature Optimization Process}
    \begin{algorithmic}[1]
        \State \textbf{Step 1:} Extract feature sets with varying sizes: 12, 20, 30, 40, 60, 70, 90, and 120 features.
        \State \textbf{Step 2:} Train a separate model for each feature type and feature set size.
        \State \textbf{Step 3:} For each feature set, train three different models using three classifiers: SVM, Random Forest, and Logistic Regression.
        \State \textbf{Step 4:} Evaluate the performance of each model.
    \end{algorithmic}
    \end{algorithm}

Figure \ref{fig:n_feature_per_type} shows the results obtained for each type of feature with the Random Forest model, which significantly outperformed the other classifiers.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{../images/n_feature_per_type.png}
    \caption{F1 score per number of features}
    \label{fig:n_feature_per_type}
\end{figure*}

\subsection{Results and Analysis}

\textbf{MFCC:} The MFCC features consistently achieved the highest F1 scores, peaking around 0.7. This indicates their effectiveness for the classification task. Interestingly, the number of MFCC features (ranging from 30 to 120) did not drastically affect performance, suggesting that even a smaller set of MFCC features can be highly informative.

\textbf{CQT:} The CQT features showed moderate performance, with F1 scores around 0.4 to 0.5. The optimal number of features was around 70, beyond which there was no significant improvement.

\textbf{RMS:} RMS features exhibited F1 scores ranging from 0.4 to 0.5, with optimal performance achieved with around 70 features.

\textbf{ZCR, Spectral Centroid (SC), Spectral Bandwidth (SB), and Spectral Roll-off (SR):} The F1 scores for these features generally stabilized around 0.4 to 0.5. Increasing the number of features beyond 40 did not result in significant performance gains and could even degrade the model's performance. This suggests that adding too many features, especially those without strong predictive power, can confuse the model and degrade its performance.

\subsection{Optimal Number of Features}

Based on the results, the optimal number of features for each type is as follows:
\rowcolors{2}{blue!8}{blue!18}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Feature Type} & \textbf{Optimal Number of Features} \\
        \hline
        MFCC & 30 \\
        Chroma & 12 \\
        CQT & 70 \\
        RMS & 40 \\
        ZCR & 40 \\
        Spectral Centroid & 40 \\
        Spectral Bandwidth & 60 \\
        Spectral Roll-off & 40 \\
        \hline
    \end{tabular}
    \caption{Optimal number of features for each type}
    \label{tab:optimal_features}
\end{table}


\section{Correlation Analysis}

\subsection{Feature Correlation Analysis}

Given the large number of features (338 in total), it was necessary to identify and remove features that are poorly correlated with the target variable as well as those that are highly correlated with each other. Due to the high number of features, a visual approach, such as a correlation matrix, was not feasible. Instead, two filters were applied to select the most relevant features using the Spearman correlation coefficient, as the normality test failed.

\begin{algorithm}
    \caption{Feature Selection Process}
    \begin{algorithmic}[1]
        \State \textbf{Step 1:} Calculate the Spearman correlation coefficient for each feature with the target variable.
        \State \textbf{Step 2:} Apply the first filter to remove features with a correlation below a certain threshold with the target variable.
        \State \textbf{Step 3:} Calculate the pairwise Spearman correlation coefficients among all features.
        \State \textbf{Step 4:} Apply the second filter to remove features that have a high number of correlations (above a certain threshold) with other features.
        \State \textbf{Step 5:} Choose threshold values empirically and apply the filters using various combinations of these thresholds.
        \State \textbf{Step 6:} Train Random Forest models on the filtered data to evaluate performance and select the best combination of thresholds.
    \end{algorithmic}
    \end{algorithm}

\subsection{Threshold Selection and Model Evaluation}

Threshold values were chosen empirically and the filters were applied using the combinations shown in Table \ref{tab:threshold_values}. Using the filtered data, Random Forest models were trained and evaluated, as Random Forest was found to be the best performing model. The optimal combination of thresholds was found to be: threshold 1 = 0, threshold 2 = 0.6, resulting in 30 features.

\rowcolors{2}{blue!8}{blue!18}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Threshold} & \textbf{Values}                 \\
        THRESHOLD 1        & 0 - 0.1 - 0.2 - 0.3 - 0.4 - 0.5 \\
        THRESHOLD 2        & 0.6 - 0.7 - 0.8 - 0.9 - 1       \\
        NÂ° FEATURES        & 5 - 10 - 15 - 20 - 25 - 30 - 40 \\
        \hline
    \end{tabular}
    \caption{threshold values}
    \label{tab:threshold_values}
\end{table}

With threshold 1 = 0, the filter on the correlation between the features and the target variable was effectively bypassed. However, with threshold 2 = 0.6, a stringent filter was applied on the correlation among the features themselves, removing features that had a correlation above 0.6 with at least 30 other features. This indicates that having features highly correlated with each other is more detrimental to the model than having features poorly correlated with the target variable.

Figure \ref{fig:comparison_model_on_all_features_vs_model_on_best} shows the results obtained with the model trained on filtered features compared to the model trained on all features. As demonstrated, the model trained on filtered features performs significantly better.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{../images/model_on_all_features_vs_model_on_best.png}
    \caption{Comparison of different metrics between the model on all features and the model on the filtered ones}
    \label{fig:comparison_model_on_all_features_vs_model_on_best}
\end{figure}

\subsection{Selected Features and Correlation Matrix}

From this analysis, 41 features remained: 28 MFCC, 12 Chroma, and 1 ZCR. The correlation matrix of the filtered features is shown in Figure \ref{fig:correlation_matrix}. This matrix illustrates the pairwise correlation between the selected features, with the color intensity indicating the strength and direction of the correlation. Dark red cells represent high positive correlations, while dark blue cells indicate high negative correlations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{../images/correlation_matrix.png}
    \caption{Correlation matrix of the filtered features}
    \label{fig:correlation_matrix}
\end{figure}

The matrix demonstrates that the remaining features have low correlations with each other, as evidenced by the predominantly light colors away from the diagonal. This implies that the features are relatively uncorrelated, preventing multicollinearity issues and enhancing the robustness of the model. The high diagonal values indicate that each feature is perfectly correlated with itself, which is expected. However, the off-diagonal values being close to zero for most feature pairs confirm that the filtering process was effective in selecting features that do not exhibit high inter-correlations.
