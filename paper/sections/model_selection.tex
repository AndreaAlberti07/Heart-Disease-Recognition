\section{Models}
To effectively address the classification problem, we strategically split it into three distinct tasks:

\begin{enumerate}[label=\textbf{\arabic*.}, leftmargin=0.5cm]
    \item \textbf{Multiclass Classification:} This task addresses the broadest scope of prediction by classifying heartbeats 
    into one of the five classes present in the dataset. This is the most challenging task due to the imbalanced nature of
    the classes and the need to differentiate between multiple disease categories.
    
    \item \textbf{Normal vs Abnormal vs Artifact Classification:} This task takes a more focused approach by categorizing 
    heartbeats into three distinct classes: normal, abnormal, or artifact. It mitigates the challenge of imbalanced classes 
    present in the dataset and allows for a better understanding of the nuances within each category. Additionally, 
    it allows for a false positive rate minimization scenario, where the final goal is not the overall accuracy, but the 
    minimization of false normal predictions. This is particularly important in a medical context, where misclassifying
    an abnormal heartbeat as normal can have severe consequences.
    
    \item \textbf{Diseases-only Classification:} This task narrows the focus exclusively to the prediction of diseases, 
    excluding normal and artifact classes. It allows for a deeper exploration of the underlying diseases present in the 
    dataset while filtering out irrelevant data points. By isolating disease-related patterns and features, the model 
    can prioritize accuracy in disease prediction without the potential confounding factors introduced by normal and artifact heartbeats.
\end{enumerate}

\noindent
In all cases we trained a variety of models, with the aim of finding the best-performing model for each task.
The models used in this study are listed in Table \ref{tab:models}.

\rowcolors{2}{blue!8}{blue!18}
\begin{table}[h!]
    \centering
    \footnotesize
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Name}           & \textbf{Architecture (hidden layers)}                                 \\ \hline
	   Random Forest                 & -                                                     \\ \hline
        XGBoost                 & -                                                     \\ \hline
        CatBoost                & -                                                     \\ \hline
        LightGBM                & -                                                     \\ \hline
        MLP\_Basic              & (128, 64, 32)                                         \\ \hline
        MLP\_Ultra              & (512, 256, 128, 64, 32)                               \\ \hline
        MLP\_Large              & (256, 128, 64, 32)                                    \\ \hline
        MLP\_Small              & (64, 32)                                              \\ \hline
        MLP\_Tiny               & (32, 16)                                              \\ \hline
        MLP\_Reverse            & (32, 64, 128, 256, 512, 256, 128, 64, 32)             \\ \hline
        MLP\_Bottleneck         & (512, 64, 32)                                         \\ \hline
        MLP\_Rollercoaster      & (512, 128, 256, 128, 256, 64, 32)                     \\ \hline
        MLP\_Hourglass          & (512, 256, 128, 64, 32, 64, 128, 256, 512)            \\ \hline
        MLP\_Pyramid            & (1024, 512, 256, 128, 128, 128, 64, 32)               \\ \hline
        MLP\_Wide               & (1024, 1024)                                          \\ \hline
        MLP\_WideUltra          & (1024, 1024, 128, 32)                                 \\ \hline
        MLP\_Sparse             & (32, 16, 8)                                           \\ \hline
        MLP\_Dropout            & (128, 64, 32)                                         \\ \hline
        MLP\_Ensemble1          & MLP\_Basic, Large, Ultra                    \\ \hline
        MLP\_Ensemble2          & RandomForest, MLP\_Ultra                              \\ \hline
        MLP\_Ensemble3          & MLP\_Rollercoaster, Large                        \\ \hline
        MLP\_Ensemble4          & MLP\_Rollercoaster, Large, Ultra            \\ \hline
        MLP\_Ensemble5          & RandomForest, MLP\_Ultra, Rollercoaster          \\ \hline
        MLP\_Ensemble6          & MLP\_Rollercoaster, Large, Ultra, Wide \\ \hline
        ALL\_Ensemble           & All models majority vote                              \\ \hline
        CB\_ALL\_Ensemble       & All models CatBoost                                   \\ \hline
    \end{tabular}
    \caption{Models names and architectures.}
    \label{tab:models}
\end{table}


\subsection{Models Overview}

\subsubsection{LightGBM}
LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework that builds decision trees in a leaf-wise manner. Unlike level-wise growth used in other boosting algorithms, LightGBM grows trees leaf-wise, leading to better optimization.

The objective function to minimize is:
\begin{equation*}
L = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k} \Omega(f_k)
\end{equation*}
where $l$ is the loss function (e.g., mean squared error for regression), $y_i$ is the true label, $\hat{y}_i$ is the predicted label, and $\Omega$ is the regularization term to avoid overfitting.

\subsubsection{XGBoost}
XGBoost (Extreme Gradient Boosting) enhances the gradient boosting technique by optimizing the loss function using second-order Taylor expansion. The objective function is:
\begin{equation*}
L(\theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i(t)) + \sum_{k} \Omega(f_k)
\end{equation*}
where $l$ is the loss function, $y_i$ is the true value, $\hat{y}_i(t)$ is the prediction at the $t$-th iteration, and $\Omega(f_k)$ is the regularization term. The regularization term is given by:
\begin{equation*}
\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
\end{equation*}
where $T$ is the number of leaves, $w_j$ are the leaf weights, and $\gamma$ and $\lambda$ are regularization parameters.

\subsubsection{CatBoost}
CatBoost (Categorical Boosting) is designed to handle categorical features natively. It uses ordered boosting to avoid overfitting and provides efficient handling of categorical data without extensive preprocessing. The model learns using the following objective function:
\begin{equation*}
L = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \lambda \sum_{j=1}^{m} ||w_j||^2
\end{equation*}
where $l$ is the loss function, $y_i$ is the true label, $\hat{y}_i$ is the predicted label, $\lambda$ is the regularization parameter, and $w_j$ are the model weights.

\subsubsection{Random Forest}
Random Forest is an ensemble method that builds multiple decision trees using bootstrap samples and random feature subsets. The final prediction is made by averaging the predictions (regression) or majority voting (classification). The prediction for a regression problem is:
\begin{equation*}
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} h_t(x)
\end{equation*}
where $T$ is the number of trees, $h_t$ is the prediction of the $t$-th tree, and $x$ is the input feature vector.

\subsubsection{MLP (Multilayer Perceptron)}
MLP (Multilayer Perceptron) is a type of feedforward neural network with one or more hidden layers. Each neuron computes a weighted sum of inputs, applies an activation function, and passes the result to the next layer. The output of a neuron in layer $l$ is:
\begin{equation*}
a_i^{(l)} = f \left( \sum_{j=1}^{n^{(l-1)}} w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)} \right)
\end{equation*}
where $a_i^{(l)}$ is the activation of the $i$-th neuron in layer $l$, $w_{ij}^{(l)}$ is the weight between neuron $j$ in layer $l-1$ and neuron $i$ in layer $l$, $b_i^{(l)}$ is the bias term, and $f$ is the activation function (e.g., ReLU, sigmoid).





